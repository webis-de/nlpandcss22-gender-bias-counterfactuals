{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optional-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# nvidia-smi\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instrumental-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from flair.embeddings import FlairEmbeddings, StackedEmbeddings, ELMoEmbeddings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suitable-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('flair')\n",
    "logger.setLevel(level=logging.ERROR)\n",
    "fh = logging.StreamHandler()\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tribal-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"../data/\"\n",
    "PA_PATH = PREFIX + \"sap2017-connotation-frames-power-agency/\"\n",
    "J_PATH = PREFIX + \"pungas2017-plaintext-jokes/\"\n",
    "W_PATH = PREFIX + \"wang2018-wiki-dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-potter",
   "metadata": {},
   "source": [
    "# read power_agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prescription-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_agency = pd.read_csv(PA_PATH + \"agency_power_prepro.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-mineral",
   "metadata": {},
   "source": [
    "# general tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "circular-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SegtokSentenceSplitter()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos_tagger = SequenceTagger.load(\"upos-fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-enzyme",
   "metadata": {},
   "source": [
    "# wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac764b0-04a5-4846-981b-a71936121226",
   "metadata": {},
   "source": [
    "@inproceedings{wang-etal-2018-describing,\n",
    "    title = \"Describing a Knowledge Base\",\n",
    "    author = \"Wang, Qingyun  and\n",
    "      Pan, Xiaoman  and\n",
    "      Huang, Lifu  and\n",
    "      Zhang, Boliang  and\n",
    "      Jiang, Zhiying  and\n",
    "      Ji, Heng  and\n",
    "      Knight, Kevin\",\n",
    "    booktitle = \"Proceedings of the 11th International Conference on Natural Language Generation\",\n",
    "    month = nov,\n",
    "    year = \"2018\",\n",
    "    address = \"Tilburg University, The Netherlands\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/W18-6502\",\n",
    "    doi = \"10.18653/v1/W18-6502\",\n",
    "    pages = \"10--21\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44b95f-2836-49b3-b2ce-9400541ed817",
   "metadata": {},
   "source": [
    "Get data at https://github.com/EagleW/Describing_a_Knowledge_Base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-today",
   "metadata": {},
   "source": [
    "## get sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inclusive-prescription",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1045758208/1045758208 [00:35<00:00, 29170197.77it/s]\n"
     ]
    }
   ],
   "source": [
    "w_texts = []\n",
    "\n",
    "with tqdm(total=os.path.getsize(W_PATH + 'wiki_person.json')) as pbar:\n",
    "    with open(W_PATH + 'wiki_person.json') as f:\n",
    "        for line in f:\n",
    "            l = json.loads(line)\n",
    "            \n",
    "            if 'TEXT' in l and len(l['TEXT']) > 0:\n",
    "                w_texts.append(l['TEXT'])\n",
    "                \n",
    "            pbar.update(len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comfortable-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_texts = [\"\".join([\" \"+i for i in s]).strip() for t in w_texts for s in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "combined-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_texts = pd.DataFrame(w_texts, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hungry-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4097687"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "narrative-syracuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Weird Al\" Yankovic \" \"Weird Al\" Yankovic \" \"W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yankovic was born in Downey, California and ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>al 's first accordion lesson which sparked his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a door-to-door salesman traveling through Lynw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yankovic claims the reason his parents chose a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  \"Weird Al\" Yankovic \" \"Weird Al\" Yankovic \" \"W...\n",
       "1  yankovic was born in Downey, California and ra...\n",
       "2  al 's first accordion lesson which sparked his...\n",
       "3  a door-to-door salesman traveling through Lynw...\n",
       "4  yankovic claims the reason his parents chose a..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "wanted-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_verbprep = power_agency[power_agency.prep.notna()][['lemma', 'prep']]\n",
    "pa_verbprep = list(pa_verbprep.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "induced-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_verblemma = set(power_agency.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "personal-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "broad-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'wsents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "continuing-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_agency[col] = [list() for x in range(len(power_agency.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "martial-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_wiki(sentence, col):\n",
    "    sent = Sentence(sentence)\n",
    "    if len(sent) > 0:\n",
    "        pos_tagger.predict(sent)\n",
    "        for token in sent:\n",
    "            if token.get_tag('pos').value == 'VERB':\n",
    "                token.add_tag('lemma', lemmatizer.lemmatize(token.text, pos='v'))\n",
    "             \n",
    "        for tid in range(len(sent)):\n",
    "            token = sent[tid]\n",
    "            if token.get_tag('pos').value == 'VERB' and token.get_tag('lemma').value in pa_verblemma:\n",
    "                # verb from pa -> has prep?\n",
    "                if tid+1 < len(sent):\n",
    "                    next_token = sent[tid+1]\n",
    "                    \n",
    "                    if (token.get_tag('lemma').value, next_token.text) in pa_verbprep:\n",
    "                        # has prep\n",
    "                        index = power_agency[(power_agency.lemma == token.get_tag('lemma').value) & (power_agency.prep == next_token.text)].index\n",
    "                    else:\n",
    "                        index = power_agency[(power_agency.lemma == token.get_tag('lemma').value) & (~power_agency.prep.notna())].index\n",
    "                else:\n",
    "                    index = power_agency[(power_agency.lemma == token.get_tag('lemma').value) & (~power_agency.prep.notna())].index\n",
    "                    \n",
    "                # add to power_agency    \n",
    "                if len(index) == 1:\n",
    "                    result = power_agency.loc[index[0], col]\n",
    "                    \n",
    "                    if len(result) < upper_bound:\n",
    "                        result.append(sent)\n",
    "                        power_agency.at[index[0], col] = result\n",
    "                else:\n",
    "                    if len(index) > 1:\n",
    "                        print(sent)\n",
    "                        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "placed-stream",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.91it/s]\n"
     ]
    }
   ],
   "source": [
    "w_texts['sent'] = w_texts['text'].progress_apply(lambda s: preprocess_wiki(s, col)) # run by python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "governmental-artist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2144.000000\n",
      "mean        1.162780\n",
      "std         5.165583\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         1.000000\n",
      "max       165.000000\n",
      "Name: wsents, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(power_agency.wsents.apply(len).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_agency.to_pickle(PA_PATH + 'power_agency_wikisents.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
