{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bigger-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# nvidia-smi\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suburban-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "figured-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('flair')\n",
    "logger.setLevel(level=logging.ERROR)\n",
    "fh = logging.StreamHandler()\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "standard-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"../data/\"\n",
    "PA_PATH = PREFIX + \"sap2017-connotation-frames-power-agency/\"\n",
    "J_PATH = PREFIX + \"pungas2017-plaintext-jokes/\"\n",
    "W_PATH = PREFIX + \"wang2018-wiki-dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-sport",
   "metadata": {},
   "source": [
    "# read power_agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "educational-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_agency = pd.read_csv(PA_PATH + \"agency_power_prepro.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-answer",
   "metadata": {},
   "source": [
    "# general tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satellite-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = SegtokSentenceSplitter()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos_tagger = SequenceTagger.load(\"upos-fast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-death",
   "metadata": {},
   "source": [
    "# jokes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f614a774-e7b7-4d9c-8481-c18b1dd1d40e",
   "metadata": {},
   "source": [
    "@misc{pungas,\n",
    "        title={A dataset of English plaintext jokes.},\n",
    "        url={https://github.com/taivop/joke-dataset},\n",
    "        author={Pungas, Taivo},\n",
    "        year={2017},\n",
    "        publisher = {GitHub},\n",
    "        journal = {GitHub repository}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eac966-e099-4062-a4c0-63daf52067dc",
   "metadata": {},
   "source": [
    "Get data at https://github.com/taivop/joke-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-elizabeth",
   "metadata": {},
   "source": [
    "## get sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "monetary-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jokes():\n",
    "    filenames = ['reddit_jokes.json', 'stupidstuff.json', 'wocka.json']\n",
    "    tags = ['body', 'title']\n",
    "    \n",
    "    result = []\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        with open(J_PATH + filename, 'r') as f: \n",
    "            result.extend(json.loads(f.read()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tutorial-ceramic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_jokes.json\n",
      "stupidstuff.json\n",
      "wocka.json\n"
     ]
    }
   ],
   "source": [
    "jokes = read_jokes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intimate-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies = [j['body'] for j in jokes]\n",
    "titles = [j['title'] for j in jokes if 'title' in j]\n",
    "j_texts = bodies + titles\n",
    "del jokes, bodies, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ahead-paraguay",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "j_texts = pd.DataFrame([j for j in j_texts if j.strip()], columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elder-territory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408163"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(j_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attractive-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_jokes(text):\n",
    "    result = []\n",
    "    sentences = splitter.split(text)\n",
    "    if len(sentences) > 0:\n",
    "        for sent in sentences:\n",
    "            if len(sent) > 0:\n",
    "                pos_tagger.predict(sent)\n",
    "                for token in sent:\n",
    "                    if token.get_tag('pos').value == 'VERB':\n",
    "                        token.add_tag('lemma', lemmatizer.lemmatize(token.text, pos='v'))\n",
    "                result.append(sent)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_texts['sent'] = j_texts['text'].progress_apply(preprocess_jokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f000ad-f207-4ca4-8a05-fa34aa9917c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "j_texts['sent'].to_pickle(J_PATH + 'j_sents.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-photography",
   "metadata": {},
   "source": [
    "## filter sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incident-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = pd.read_pickle(J_PATH + 'j_sents.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fitted-punishment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>agency</th>\n",
       "      <th>power</th>\n",
       "      <th>verb_prep</th>\n",
       "      <th>prep</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandons</td>\n",
       "      <td>agency_pos</td>\n",
       "      <td>power_agent</td>\n",
       "      <td>abandons</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abolishes</td>\n",
       "      <td>agency_pos</td>\n",
       "      <td>power_agent</td>\n",
       "      <td>abolishes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absorbs</td>\n",
       "      <td>agency_pos</td>\n",
       "      <td>power_agent</td>\n",
       "      <td>absorbs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>absorb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abuses</td>\n",
       "      <td>agency_pos</td>\n",
       "      <td>power_agent</td>\n",
       "      <td>abuses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>abuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accelerates</td>\n",
       "      <td>agency_pos</td>\n",
       "      <td>power_agent</td>\n",
       "      <td>accelerates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>accelerate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          verb      agency        power    verb_prep prep       lemma\n",
       "0     abandons  agency_pos  power_agent     abandons  NaN     abandon\n",
       "1    abolishes  agency_pos  power_agent    abolishes  NaN     abolish\n",
       "2      absorbs  agency_pos  power_agent      absorbs  NaN      absorb\n",
       "3       abuses  agency_pos  power_agent       abuses  NaN       abuse\n",
       "4  accelerates  agency_pos  power_agent  accelerates  NaN  accelerate"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_agency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "changing-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_verbprep = power_agency[power_agency.prep.notna()][['lemma', 'prep']]\n",
    "pa_verbprep = list(pa_verbprep.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "searching-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_verblemma = set(power_agency.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extended-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "thorough-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_verb(sents, col):\n",
    "    for sent in sents:  \n",
    "        for tid in range(len(sent)):\n",
    "            token = sent[tid]\n",
    "            if token.get_tag('pos').value == 'VERB' and token.get_tag('lemma').value in pa_verblemma:\n",
    "                # verb from pa -> has prep?\n",
    "                if tid+1 < len(sent):\n",
    "                    next_token = sent[tid+1]\n",
    "                    \n",
    "                    if (token.get_tag('lemma').value, next_token.text) in pa_verbprep:\n",
    "                        # has prep\n",
    "                        index = power_agency[(power_agency.lemma == token.get_tag('lemma').value) & (power_agency.prep == next_token.text)].index\n",
    "                    else:\n",
    "                        index = power_agency[(power_agency.lemma == token.get_tag('lemma').value) & (~power_agency.prep.notna())].index\n",
    "                else:\n",
    "                    index = power_agency[(power_agency.lemma == token.get_tag('lemma').value) & (~power_agency.prep.notna())].index\n",
    "                    \n",
    "                # add to power_agency    \n",
    "                if len(index) == 1:\n",
    "                    result = power_agency.loc[index[0], col]\n",
    "                    \n",
    "                    if len(result) < upper_bound:\n",
    "                        result.append(sent)\n",
    "                        power_agency.at[index[0], col] = result\n",
    "                else:\n",
    "                    if len(index) > 1:\n",
    "                        print(sent)\n",
    "                        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "burning-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'jsents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confirmed-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_agency[col] = [list() for x in range(len(power_agency.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "separate-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408163/408163 [30:51<00:00, 220.48it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         None\n",
       "1         None\n",
       "2         None\n",
       "3         None\n",
       "4         None\n",
       "          ... \n",
       "408158    None\n",
       "408159    None\n",
       "408160    None\n",
       "408161    None\n",
       "408162    None\n",
       "Name: sent, Length: 408163, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents.progress_apply(lambda s: add_to_verb(s, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "indonesian-timeline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2144.000000\n",
       "mean      115.518190\n",
       "std       113.048178\n",
       "min         0.000000\n",
       "25%        22.000000\n",
       "50%        61.000000\n",
       "75%       230.000000\n",
       "max       300.000000\n",
       "Name: jsents, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_agency.jsents.apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "married-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_agency.to_pickle(PA_PATH + 'power_agency_jokesents.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-single",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
